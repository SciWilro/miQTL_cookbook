---
title: "CHARGE miQTL pipeline"
author: "Alexander Kurilshikov"
date: "Aug 7, 2017"
output: html_document
---

## Contacts

If you have any questions and suggestions, don't hesitate to write us: Alexander Kurilshikov (alexa.kur@gmail.com), Sasha Zhernakova (sasha.zhernakova@gmail.com)

# Overall description

This is the cookbook for performing the GWAS analysis of microbial abundance based on analysis of 16S rRNA sequencing dataset. It includes 4 major steps:

1. [Processing of 16S data](#chapter-1-16s-data-processing)
2. [Processing of SNP microarray data](#chapter-2-genotype-imputation)
3. [Performing the association study](#chapter3-peform-genome-wide-association-study)
4. Making the meta-analysis

Steps from 1 to 3 would be performed in-house by every participating group. Step 4 will be performed in UMCG (Groningen). Mostly, you can just copy the code strings from this cookbook and run them, but sometimes it's not the case (yet, but it will be after beta-testing).

#### Genome-Wide Association Study itself will be performed according to this design:

1. Cutoffs and transformations
    + Taxonomies:
        + Abundance cutoff: presence in 10% of the samples
        + Log (base **e**) trasformation on the counts
    + SNPs
        + MAF > 1%
        + Imputation quality > 0.4
        + Genotypes represented in dosages 
2. Models used
    + Taxonomy absence/presence as binary trait: logistic regression with Chisquared-based p-value estimation
    + For non-zero samples: linear regression model on log-transformed counts with Fisher test-based p-value estimation
3. Meta-analysis
    + will be performed separately, for binary and quantitative models

# Chapter 1. 16S data processing

For 16S analysis, RDP Classifier will be used instead of OTU picking, since it has shown more consistent results between different domains. According to the pipeline, Genome-Wide Association Study will be performed for the following taxonomic levels:

1. Taxonomic levels:
    + Classes
    + Orders
    + Phyla
    + Families
    + Genera
2. Cutoffs:
    + Taxonomy should be presented in more than 10% of the samples in cohort
3. Transformations:
    + Taxonomies counts should be log-transformed (on the base ***e***)

## 1.1. 16S data requirements

1. Sequence quality-based read filtering should be performed before rarefaction.
2. Reads should be randomly rarefied to 10,000 reads per sample before OTU picking (See [Appendix2](#Appendix2) )
3. Sequences from all samples should be merged into one file.
4. For every sequence, FASTA header should follow this format:

```
>[SampleID]_[SequenceID] [OptionalInfo]
```

Example of valid fasta record:

```
>G36899_G36899.A2137
TACGTAGGGGGCAAGCGTTATCCGGATTTACTGGGTGTAAAGGGAGCGTAGACGGACTGG
CAAGTCTGATGTGAAAGGCGGGGGCTCAACCCCTGGACTGCATTGGAAACTGTTAGTCTT
GAGTGCCGGAGAGGTAAGCGGAATTCCTAGTGTAGCGGTGAAATGCGTAGATATTAGGAG
GAGCACCAGTGGCGAAGGCGGCTTACTGGACGGTAACTGACGTTGAGGCTCGAAAGCGTG
GGGAGCAAACAGG
```

For this record, SampleID is **G36899**, and SequenceID is **G36899.A2137.451**.

## 1.2. Software and database installation

These software and databases are necessary:

1. Java
2. R
3. RDP Classifier 2.12. Can be downloaded [here](https://sourceforge.net/projects/rdp-classifier/)

You also need reference database for RDP Classifier and some additional scripts. They are included in this GitHub page, and located in the folders [database](https://github.com/alexa-kur/miQTL_cookbook/tree/master/database) and [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software). The list of things you need to get here includes:

4. SILVA 128 release database prepared for RDP Classifier. available on this Github in the folder named [database](https://github.com/alexa-kur/miQTL_cookbook/tree/master/database). It's a spanned zip archive, so you first need to unzip it. It's also available on the consortium [Dropbox](https://www.dropbox.com/home/Microbiome-QTL_Charge): located in the folder [CookBook_CHARGE_miQTL/database](https://www.dropbox.com/home/Microbiome-QTL_Charge/CookBook_CHARGE_miQTL/database) and named *silva128_rdpDB.zip*. 
5. RDP output parsing script. Located in the folder [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software) and named **step1.2_rdp_process.sh**.
6. Script for generating summary statistics **step1.3_generate_summary.R**. Located in the folder [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software)
7. Filtering and transformation script **step1.4_filter_and_transform.R**. Located in the folder [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software)

Before usage, you can put the scripts and 16S database folter to your project folder. 

## 1.3. Running data processing. 

### Step 1.1. Taxonomy binning. 

Go to your project folder. Replace SEQUENCES.FASTA to the filename of your 16S sequences and run:

```
java -Xmx10G -jar ./rdp_classifier_2.12/dist/classifier.jar -t ./SILVA128_rdpDB/rRNAClassifier.properties -o results.out SEQUENCES.FASTA
````

File **results.out** will be generated. Please note that this step is time consuming and can take up to several days. 


### Step 1.2. Process mapping results.

```
bash step1.2_rdp_process.sh results.out 0.8
````

It will generate file **taxonomy_table.txt**, tab-separated table which contains per-sample counts of taxa in your dataset. 


### Step 1.3. Generate summary statistics

Please run this code and send us the results file **COHORT_NAME_summary_16s.txt** (replace COHORT_NAME to the real name of your cohort)

```
Rscript step1.3_generate_summary.R taxonomy_table.txt COHORT_NAME_summary_16s.txt
```

### Step 1.4. Abundance filtering and transformation

The script *step1.4_filter_and_transform.R* removes taxa presented in less than 10% of samples and performes log transformation 
```
Rscript step1.4_filter_and_transform.R taxonomy_table.txt tax_filtered_logTrans.txt 0.1
```

**tax_filtered_logTrans.txt** fill be generated. This file will be used in [miQTL mapping](#chapter3-peform-genome-wide-association-study) .  

# Chapter 2. Genotype imputation

To simplify the analysis, we should have genotypes harmonized between cohorts. We propose to use one imputation server: [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html). 
Carolina Medina-Gomez was very pleased to prepase very nice instruction for imputation, please find here, in the file called [Imputation_protocol_description.docx](Imputation_protocol_description.docx). In addition to that, you can also check the documentation on HRC [server help page](https://imputationserver.sph.umich.edu/index.html#!pages/help).

# Chapter3. Peform Genome-Wide Association Study

In this section, we will perform the following steps:

1. [Correct microbiome data for covariates](#step-31-correct-microbiome-for-covariates-and-create-annotation-file)
2. [Process HRC VCF files](#step-32-process-hrc-vcf-files)
3. [Prepare coupling file](#step-33-prepare-coupling-file)
4. [Run miQTL mapping](#step-34-run-miqtl-mapping)

All necessary scripts are probided in [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software) folder. Thus, it includes:

1. Script **step3.1_correct_for_covariates.R**
2. **GenotypeHarmonizer**
3. **eQTL-pipeline**

## Step 3.1. Correct microbiome for covariates and create annotation file

First, you need to generate filtered and log-transformed taxonomy table *tax_filtered_logTrans.txt*. Please check [Step 1.4](#step-14-abundance-filtering-and-transformation) to explore how to do it. Now you need to create the flat text table with phenotypes you want to correct for. It should be tab-delimited flat text file which suits for loading in R. Phenotypes can be integer or numeric variables or factors. The first column should contain Sample ID's. The typical example is provided below: 

```
SampleID  Sex   Age
Sample1   0       23
Sample2   1       74
Sample3   0       58
Sample4   1       76
```

This also works (column name for SampleID's can be skipped)

```
Sex   Age
Sample1   0       23
Sample2   1       74
Sample3   0       58
Sample4   1       76
```

If your sample names are **different** between taxonomy table and phenotype file, please add **linkage file**. This is an example of linkage file:

```
PhenotypeSample1  StoolSample1
PhenotypeSample2  StoolSample2
```

Phenotype ID comes first, followed by stool sample ID (again, it's tab-delimited file). 

Now run (assuming your phenotype file is called **phenotypes.txt**)

```
#If your sample ID's are matched
Rscript step3.1_correct_for_covariates.R tax_filtered_logTrans.txt phenotypes.txt tax_corrected.txt

#If your sample ID's are different between phenotypes and 16s libraries
Rscript step3.1_correct_for_covariates.R tax_filtered_logTrans.txt phenotypes.txt tax_corrected.txt linkage.txt
```

The file **tax_corrected.txt** will be generated. Now you need to create annotation for taxonomy table. From the folder your **tax_corrected.txt** is located, run: 

```
Rscript step.3.1.1_create_annotation.R tax_corrected.txt tax_annotation.txt
```



## Step 3.2. Process HRC VCF files

First, you need to install **GenotypeHarmonizer** software. simply untar the archive [GenotypeHarmonizer-1.4.20-dist.tar.gz](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software/GenotypeHarmonizer-1.4.20-dist.tar.gz) from [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software) folder:

```
tar -zxf GenotypeHarmonizer-1.4.20-dist.tar.gz
``` 

Please check the structure of the genotype folder you downloaded from HRC. It should be per-chromosome gzipped VCF files:

```
chr10.dose.vcf.gz
chr10.dose.vcf.gz.md5
chr10.dose.vcf.gz.tbi
chr10.dose.vcf.gz.tbi.md5
chr10.info.gz
chr10.info.gz.md5
chr11.dose.vcf.gz
chr11.dose.vcf.gz.md5
chr11.dose.vcf.gz.tbi
chr11.dose.vcf.gz.tbi.md5
...
```

Keep only autosomal data in the folder (assuming your HRC folder is named **HRC_VCF**):

```
mkdir autosomal_genotypes
cp HRC_VCF/chr[0-9]* autosomal_genotypes
```

Next command will perform the following steps:
1. MAF filtering (*-mf* flag, we decided to use 0.01 threshold)
2. Imputation QC filtering (*-ip* flag, 0.4)
3. Remove ambigous SNPs (*-asf* flag)
4. Translate genotypes from VCF to TRITYPER format

To apply these procedures, run:
```
java -Xmx40G -jar ./GenotypeHarmonizer-1.4.20-SNAPSHOT/GenotypeHarmonizer.jar -i ./autosomal_genotypes -I VCFFOLDER -o genotypes_trityper -O TRITYPER -mf 0.01 -ip 0.4 -asf  
```

The folder **genotypes_trityper** will be generated.

For further information on *GenotypeHarmonizer*, please check its [GitHub](https://github.com/molgenis/systemsgenetics/wiki/Genotype-Harmonizer) or just contact me (alexa.kur@gmail.com).


## Step 3.3. Prepare coupling file

This file is necessary to run the pipeline. It should contain linkages between genotype and microbiome sample names. It's necessary even if you have the same names (in this case, just make two exact columns). It should be tab-delimited flat text file with two columns:

```
GenotypeSample1  StoolSample1
GenotypeSample2  StoolSample2
```

## Step 3.4. Run miQTL mapping

First you need to unzip eQTL pipeline binaries: [eqtl-mapping-pipeline-1.4nZ-dist.zip](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software/eqtl-mapping-pipeline-1.4nZ-dist.zip) from [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software). Simply run:

```
unzip -r eqtl-mapping-pipeline-1.4nZ-dist.zip
```

Our eQTL pipeline requires all the configuraiton written in XML format. The [template XML](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software/template.xml) can be downloaded from [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software). 

If you used standard file and folder names mentioned in this cookbook, you don't need to modify template, just copy it into the project folder and run the pipeline. So, these are the files that should be located in project folder:

1. **tax_corrected.txt**  // The microbiome file which contains filtered, transformed and covariate-corrected microbial taxa. See [Chapter 1](#chapter-1-16s-data-processing) and [Step 3.1.](#step-31-correct-microbiome-for-covariates-and-create-annotation-file)
2. **tax_annotation.txt** //Annotation file for taxonomies. See [Step 3.1.](#step-31-correct-microbiome-for-covariates-and-create-annotation-file)
3. **genotypes_trityper** // The folder which contains MAF and QC filtered autosomal genotypes in TriTyper format. See [Step 3.2](#step-32-process-hrc-vcf-files)
4. **coupling_file.txt** // Coupling file. See [Step 3.3.](#step-33-prepare-coupling-file)
5. **eqtl-mapping-pipeline-1.4nZ** // The folder with eQTL pipeline binaries
6. **template.xml** // XML template from [software](https://github.com/alexa-kur/miQTL_cookbook/tree/master/software) folder. 

If you have all of these ready, jun run this from a *screen* session:

```
java -Xmx30G eqtl-mapping-pipeline-1.4nZ/eqtl-mapping-pipeline.jar --mode metaqtl --settings template.xml 
```

Now you are done!



# Additions

## Appendix 1. OTU-based 16S data processing

For those who are still interested in performing 16S processing for their own purposes, we provide the OTU-based pipeline. Please note that for miQTL meta-analysis we require using RDPclassifer pipeline described in section 1!

### Installation

First we need to install QIIME on your machine. I should be any kind of Linux machine, including physical or virtual machine. You don't need to have root priviliges to use QIIME

```{bash,eval = FALSE}
cd ~/
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod a+x Miniconda3-latest-Linux-x86_64.sh
bash ./Miniconda3-latest-Linux-x86_64.sh
#follow installation instructions and install Miniconda in ~/miniconda3 folder
conda create -n qiime1 python=2.7 qiime matplotlib=1.4.3 mock nose -c bioconda
source activate qiime1
#check if installation was finished succesfully
print_qiime_config.py
```

### SILVA database downloading

```{bash,eval = FALSE}
#create project dir
mkdir 16S_picking
#copy your 16S data in the folder
cp PATH_TO_YOUR_FASTA_FILE ./16S_picking
cd 16S_picking

#download SILVA v.119 database
wget https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_119_release.zip
unzip Silva_119_release.zip
#create custom config file
echo "\
pick_otus_reference_seqs_fp $PWD/Silva119_release/rep_set/97/Silva_119_rep_set97.fna
pynast_template_alignment_fp $PWD/Silva119_release/core_alignment/core_Silva119_alignment.fna
assign_taxonomy_reference_seqs_fp $PWD/Silva119_release/rep_set/97/Silva_119_rep_set97.fna
assign_taxonomy_id_to_taxonomy_fp $PWD/Silva119_release/taxonomy/97/taxonomy_97_7_levels.txt" > qiime_config
```

### Running the OTU picking (closed OTU picking method with 97% cutoff)

```{bash,eval = FALSE}
# if you have multiple cores, you can run analysis faster in parallel, redefining the $PROCNUM varialbe 
PROCNUM=1
source activate qiime1
export QIIME_CONFIG_FP=$PWD/qiime_config
pick_closed_reference_otus.py -i FASTA_FILE -o RESULT_FOLDER -a -O $PROCNUM
cd RESULT_FOLDER
biom convert -i otu_table.biom -o otu_table.tsv --to-tsv --header-key taxonomy
cat otu_table.tsv|tail -n+2 |perl -pe "s/#OTU ID/OTU_ID/" > temp.tsv
mv temp.tsv otu_table.tsv
```

### Getting taxonomies from OTU table

For this step, you should have R to be installed. 

```{r, eval = FALSE}
cutoff_presence = 0.1
get_taxonomy_table = function(otu_table, replace_string,cutoff = 0.1){
  otu_notax = as.matrix(otu_table[,-ncol(otu_table)])
  taxonomy = sub(replace_string,"",otu_table[,ncol(otu_table)])
  dnew = aggregate(otu_notax ~ as.factor(taxonomy),FUN = sum)
  rownames(dnew) = as.character(dnew[,1])
  dnew = dnew[,-1]
  dnew = t(dnew)
  dnew
  filter = dnew[,(colSums(dnew > 0) > 0.1*nrow(dnew))]
  return(dnew)
}
otus = read.table("otu_table.tsv",header=T,row.names=1,sep="\t",as.is = T)
metadata = data.frame(tax = c("genus","family","order","class"),replace_string = c("; D_6.*","; D_5.*","; D_4.*","; D_3.*"))
result = list()
for (i in 1:nrow(metadata)){
  taxonomy_table = get_taxonomy_table(otus,replace_string = metadata[i,2])
  result[[i]] = taxonomy_table
}
final_table = cbind(result[[1]],result[[2]],result[[3]],result[[4]])
final_table = final_table[,(colSums(final_table>0)>cutoff_presence * nrow(final_table))]
final_ln = log(final_table)
final_table[final_table == "-Inf"] = NA
write.table(final_table,file = "microbes.txt",sep="\t")
```

## Appendix 2. FASTA file rarefaction

We recommend to perform rarefaction before running the pipeline. To make it truly random, this procedure can be applied:

1. Install QIIME (see Appendix 1)
2. Create merged file for all samples with number of reads larger than threshold you want 
2. Check if your fasta headers are formatted properly (see section 1)
2. Download script **run_rarefaction.R** from Consofrtium dropbox [CookBook_CHARGE_miQTL/software](https://www.dropbox.com/home/Microbiome-QTL_Charge/CookBook_CHARGE_miQTL/software)
3. Put it into your project folder. Replace SEQUENCES.FASTA to the file where you store your sequences and run:

```{}
Rscript step0.2_run_rarefaction.R SEQUENCES.FASTA 10000
source activate qiime1
filter_fasta.py -f SEQUENCES.FASTA -o SEQ_RARIFIED.FASTA -s 2filter.ids
rm 2filter.ids

```

It will generate rarefied fasta file called **SEQ_RARIFIED.FASTA**

