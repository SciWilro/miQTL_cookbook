---
title: "CHARGE bQTL pipeline"
author: "Alexander Kurilshikov"
date: "January 20, 2017"
output: html_document
---

# Overall description

To perform GWAS for the microbiome traits, we should pass 4 steps, including:

1. Processing of 16S data
2. Processing of SNP microarray data
3. Performing association study
4. Making meta-analysis

Steps from 1 to 3 would be performed in-house. Step 4 will be performed in UMCG (Groningen) and Rotterdam MC

# Chapter 1. 16S data processing

## 1.1. 16S data requirements

1. Sequence quality-based read filtering should be performed before rarefaction.
2. Reads should be randomly rarefied to 10,000 reads per sample before OTU picking.
3. Sequences from all samples should be merged into one file.
4. For every sequence, FASTA header should follow this format:

```{bash, eval = FALSE}
>[SampleID]_[SequenceID] [OptionalInfo]
```

Example of valid fasta record:

```{bash, eval = FALSE}
>G36899_G36899.A2137
TACGTAGGGGGCAAGCGTTATCCGGATTTACTGGGTGTAAAGGGAGCGTAGACGGACTGG
CAAGTCTGATGTGAAAGGCGGGGGCTCAACCCCTGGACTGCATTGGAAACTGTTAGTCTT
GAGTGCCGGAGAGGTAAGCGGAATTCCTAGTGTAGCGGTGAAATGCGTAGATATTAGGAG
GAGCACCAGTGGCGAAGGCGGCTTACTGGACGGTAACTGACGTTGAGGCTCGAAAGCGTG
GGGAGCAAACAGG
```

For this record, SampleID is **G36899**, and SequenceID is **G36899.A2137.451**.

## Software and database installation

These software and databases are necessary:

1. Java
2. R
3. RDP Classifier 2.12. Can be downloaded from [here](https://sourceforge.net/projects/rdp-classifier/)
4. SILVA 128 release database prepared for RDP Classifier. Available on the consortium [Dropbox](https://www.dropbox.com/home/Microbiome-QTL_Charge). Located in [this folder](https://www.dropbox.com/home/Microbiome-QTL_Charge/CookBook_CHARGE_miQTL/database) and named *silva128_rdpDB.zip*.
5. RDP output parsing script. Available on the consortium [Dropbox](https://www.dropbox.com/home/Microbiome-QTL_Charge). Located in [this folder](https://www.dropbox.com/home/Microbiome-QTL_Charge/CookBook_CHARGE_miQTL/software/scripts) and named *rdp_process.sh*.

# Chapter 2. Genotype imputation

To simplify the analysis, we should have genotypes harmonized between cohorts. We propose to use one imputation server: [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html). All imputation steps are nicely described on the [server help page](https://imputationserver.sph.umich.edu/index.html#!pages/help), so in this guideline we just mention crucial steps.

## Data preparation

### harmonize your chip data to be consistent with HRC imputation database

There are several tools to check the consistency between user's microarray data and HRC reference. They can be found on [this page](http://www.well.ox.ac.uk/~wrayner/tools/)

```{bash, eval = FALSE}
#download checking tool
wget http://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim.v4.2.5.zip
unzip HRC-1000G-check-bim.v4.2.5.zip
#download HRC tab file
wget ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz
gunzip HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz
#preprare freq file necessary for the script
plink --bfile <BED_FILE> --freq --out <FREQ_FILE>
#run checking tool
perl HRC-1000G-check-bim-v4.2.pl -b <BED_FILE> -f <FREQ_FILE> -r HRC.r1-1.GRCh37.wgs.mac5.sites.tab -h

```

### Prepare input files
This server allows anly bgzip-compressed VCF files as an input, so you should translate your PED files to *per choromosome* VCF's. These packages are necessary to do this: [plink2](https://www.cog-genomics.org/plink2/), [VCFtools](http://vcftools.sourceforge.net/), [tabix with bgzip](http://sourceforge.net/projects/samtools/files/tabix/) and [checkVCF](https://github.com/zhanxw/checkVCF) 

```{bash, eval = FALSE}
plink --ped mystudy_chr1.ped --map mystudy_chr1.map --recode vcf --out mystudy_chr1
vcf-sort mystudy_chr1.vcf | bgzip -c > mystudy_chr1.vcf.gz
```

That's possible to get some statistics for your VCF's using simple script

```{bash, eval = FALSE}
mkdir checkVCF & cd checkVCF
wget http://qbrc.swmed.edu/zhanxw/software/checkVCF/checkVCF-20140116.tar.gz
tar zxf checkVCF-20140116.tar.gz
cd ../
python checkVCF/checkVCF.py -r checkVCF/hs37d5.fa -o test <VCF_FILE>
```

## send data to the server

Follow the [HRC instructions](https://imputationserver.sph.umich.edu/index.html#!pages/help) to register, upload your VCF files and download the results 

## perform post-imputation check

There is a special tool [Imputation Checker](http://www.well.ox.ac.uk/~wrayner/tools/Post-Imputation.html) which allows to get post-imputation summary statistics. It depends on *libgd* and *Perl GD::Graph* so first check if you have these libraries installed. See documentation to this tool.

```{bash, eval = FALSE}
wget http://www.well.ox.ac.uk/~wrayner/tools/ic.v1.0.2.zip
wget http://www.well.ox.ac.uk/~wrayner/tools/vcfparse.zip
unzip ic.v1.0.2.zip
unzip vcfparse.zip
./vcfparse.pl -d <directory of VCFs> -o <outputname>
./ic -d <directory> -r r1.1  -h -o <OUTPUT>
```

# Chapter3. Peform Genome-Wide Association Study

IN PROGRESS

# Appendix 1. OTU-based 16S data processing

For those who are still interested in performing 16S processing for their own purpose, we provide the QIIME-based pipeline. Please note that for miQTL meta-analysis we strongly recommend to use RDPclassifer pipeline described in section 1!

## Installation

First we need to install QIIME on your machine. I should be any kind of Linux machine, including physical or virtual machine. You don't need to have root priviliges to use QIIME

```{bash,eval = FALSE}
cd ~/
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod a+x Miniconda3-latest-Linux-x86_64.sh
bash ./Miniconda3-latest-Linux-x86_64.sh
#follow installation instructions and install Miniconda in ~/miniconda3 folder
conda create -n qiime1 python=2.7 qiime matplotlib=1.4.3 mock nose -c bioconda
source activate qiime1
#check if installation was finished succesfully
print_qiime_config.py
```

## SILVA database downloading

```{bash,eval = FALSE}
#create project dir
mkdir 16S_picking
#copy your 16S data in the folder
cp PATH_TO_YOUR_FASTA_FILE ./16S_picking
cd 16S_picking

#download SILVA v.119 database
wget https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_119_release.zip
unzip Silva_119_release.zip
#create custom config file
echo "\
pick_otus_reference_seqs_fp $PWD/Silva119_release/rep_set/97/Silva_119_rep_set97.fna
pynast_template_alignment_fp $PWD/Silva119_release/core_alignment/core_Silva119_alignment.fna
assign_taxonomy_reference_seqs_fp $PWD/Silva119_release/rep_set/97/Silva_119_rep_set97.fna
assign_taxonomy_id_to_taxonomy_fp $PWD/Silva119_release/taxonomy/97/taxonomy_97_7_levels.txt" > qiime_config
```

## Running the OTU picking (closed OTU picking method with 97% cutoff)

```{bash,eval = FALSE}
# if you have multiple cores, you can run analysis faster in parallel, redefining the $PROCNUM varialbe 
PROCNUM=1
source activate qiime1
export QIIME_CONFIG_FP=$PWD/qiime_config
pick_closed_reference_otus.py -i FASTA_FILE -o RESULT_FOLDER -a -O $PROCNUM
cd RESULT_FOLDER
biom convert -i otu_table.biom -o otu_table.tsv --to-tsv --header-key taxonomy
cat otu_table.tsv|tail -n+2 |perl -pe "s/#OTU ID/OTU_ID/" > temp.tsv
mv temp.tsv otu_table.tsv
```

## Getting taxonomies from OTU table

For this step, you should have R to be installed. 

```{r, eval = FALSE}
cutoff_presence = 0.1
get_taxonomy_table = function(otu_table, replace_string,cutoff = 0.1){
  otu_notax = as.matrix(otu_table[,-ncol(otu_table)])
  taxonomy = sub(replace_string,"",otu_table[,ncol(otu_table)])
  dnew = aggregate(otu_notax ~ as.factor(taxonomy),FUN = sum)
  rownames(dnew) = as.character(dnew[,1])
  dnew = dnew[,-1]
  dnew = t(dnew)
  dnew
  filter = dnew[,(colSums(dnew > 0) > 0.1*nrow(dnew))]
  return(dnew)
}
otus = read.table("otu_table.tsv",header=T,row.names=1,sep="\t",as.is = T)
metadata = data.frame(tax = c("genus","family","order","class"),replace_string = c("; D_6.*","; D_5.*","; D_4.*","; D_3.*"))
result = list()
for (i in 1:nrow(metadata)){
  taxonomy_table = get_taxonomy_table(otus,replace_string = metadata[i,2])
  result[[i]] = taxonomy_table
}
final_table = cbind(result[[1]],result[[2]],result[[3]],result[[4]])
final_table = final_table[,(colSums(final_table>0)>cutoff_presence * nrow(final_table))]
final_ln = log(final_table)
final_table[final_table == "-Inf"] = NA
write.table(final_table,file = "microbes.txt",sep="\t")
```

